{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from timeseries_fastai.core import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastcore.all import *\n",
    "from fastai.basics import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries Models\n",
    "> A list of timeseries neural architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.rand(2, 1, 20) # batch of 16 items with 1 channels and lenght 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "act_fn = nn.ReLU(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `AdaptiveConcatPool2d` but on 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveConcatPool1d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`\"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (Linear layered model)\n",
    "A simple model builder to create a bunch of `BatchNorm1d`, `Dropout` and `Linear` layers, with `act_fn` activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_mlp(ni, nout, linear_sizes=[500, 500, 500]):\n",
    "    layers = []\n",
    "    sizes = zip([ni]+linear_sizes, linear_sizes+[nout])\n",
    "    for n1, n2 in sizes:\n",
    "            layers += LinBnDrop(n1, n2, p=0.2, act=act_fn if n2!=nout else None)\n",
    "    return nn.Sequential(Flatten(),\n",
    "                         *layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(full=False)\n",
       "  (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): Dropout(p=0.2, inplace=False)\n",
       "  (3): Linear(in_features=20, out_features=500, bias=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): Dropout(p=0.2, inplace=False)\n",
       "  (7): Linear(in_features=500, out_features=500, bias=False)\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): Dropout(p=0.2, inplace=False)\n",
       "  (11): Linear(in_features=500, out_features=500, bias=False)\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): Dropout(p=0.2, inplace=False)\n",
       "  (15): Linear(in_features=500, out_features=37, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = create_mlp(1*20, 37)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(ts).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Network (FCN).\n",
    "> A bunch of convolutions stacked together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_fcn(ni, nout, ks=9, conv_sizes=[128, 256, 128], stride=1):\n",
    "    layers = []\n",
    "    sizes = zip([ni]+conv_sizes, conv_sizes)\n",
    "    for n1, n2 in sizes:\n",
    "            layers += [ConvLayer(n1, n2, ks=ks, ndim=1, stride=stride)]\n",
    "    return nn.Sequential(*layers, \n",
    "                         AdaptiveConcatPool1d(),\n",
    "                         Flatten(),\n",
    "                         *LinBnDrop(2*n2, nout)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvLayer(\n",
       "    (0): Conv1d(1, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (1): ConvLayer(\n",
       "    (0): Conv1d(128, 256, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (2): ConvLayer(\n",
       "    (0): Conv1d(256, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (3): AdaptiveConcatPool1d(\n",
       "    (ap): AdaptiveAvgPool1d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (4): Flatten(full=False)\n",
       "  (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): Linear(in_features=256, out_features=37, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn = create_fcn(1, 37)\n",
    "fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn(ts).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def res_block_1d(nf, ks=[5,3]):\n",
    "    \"Resnet block as described in the paper.\"\n",
    "    return SequentialEx(ConvLayer(nf, nf, ks=ks[0], ndim=1, ),\n",
    "                        ConvLayer(nf, nf, ks=ks[1], ndim=1, act_cls=None),\n",
    "                        MergeLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialEx(\n",
       "  (layers): ModuleList(\n",
       "    (0): ConvLayer(\n",
       "      (0): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): ConvLayer(\n",
       "      (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): MergeLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resb = res_block_1d(16)\n",
    "resb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resb(torch.rand(2, 16, 100)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_resnet(ni, nout, kss=[9,5,3], conv_sizes=[64, 128, 128], stride=1): \n",
    "    \"Basic 11 Layer - 1D resnet builder\"\n",
    "    layers = []\n",
    "    sizes = zip([ni]+conv_sizes, conv_sizes)\n",
    "    for n1, n2 in sizes:\n",
    "            layers += [ConvLayer(n1, n2, ks=kss[0], stride=stride, ndim=1),\n",
    "                       res_block_1d(n2, kss[1:3])]\n",
    "    return nn.Sequential(*layers, \n",
    "                         AdaptiveConcatPool1d(),\n",
    "                         Flatten(),\n",
    "                        *LinBnDrop(2*n2, nout, p=0.1)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvLayer(\n",
       "    (0): Conv1d(1, 64, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (1): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): ConvLayer(\n",
       "        (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): ConvLayer(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): MergeLayer()\n",
       "    )\n",
       "  )\n",
       "  (2): ConvLayer(\n",
       "    (0): Conv1d(64, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (3): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): ConvLayer(\n",
       "        (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): ConvLayer(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): MergeLayer()\n",
       "    )\n",
       "  )\n",
       "  (4): ConvLayer(\n",
       "    (0): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (5): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): ConvLayer(\n",
       "        (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): ConvLayer(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): MergeLayer()\n",
       "    )\n",
       "  )\n",
       "  (6): AdaptiveConcatPool1d(\n",
       "    (ap): AdaptiveAvgPool1d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (7): Flatten(full=False)\n",
       "  (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): Dropout(p=0.1, inplace=False)\n",
       "  (10): Linear(in_features=256, out_features=37, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = create_resnet(1, 37)\n",
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet(ts).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Time\n",
    "> InceptionTime: Finding AlexNet for Time SeriesClassification\n",
    "The original paper repo is [here](https://github.com/hfawaz/InceptionTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is to be used within a `SequentialEx` block, it acces `orig` attribute to create the shortcut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Shortcut(Module):\n",
    "    \"Merge a shortcut with the result of the module by adding them. Adds Conv, BN and ReLU\"\n",
    "    def __init__(self, ni, nf, act_fn=act_fn): \n",
    "        self.act_fn=act_fn\n",
    "        self.conv=ConvLayer(ni, nf, ks=1, ndim=1)\n",
    "        self.bn=nn.BatchNorm1d(nf)\n",
    "    def forward(self, x): return act_fn(x + self.bn(self.conv(x.orig)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual inception module as described on the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionModule(Module):\n",
    "    \"The inception Module from `ni` inputs to len('kss')*`nb_filters`+`bottleneck_size`\"\n",
    "    def __init__(self, ni, nb_filters=32, kss=[39, 19, 9], bottleneck_size=32, stride=1):\n",
    "        if (bottleneck_size>0 and ni>1): self.bottleneck = conv(ni, bottleneck_size, 1, stride)\n",
    "        else: self.bottleneck = noop\n",
    "        self.convs = nn.ModuleList([conv(bottleneck_size if (bottleneck_size>1 and ni>1) else ni, nb_filters, ks) for ks in kss])\n",
    "        self.conv_bottle = nn.Sequential(nn.MaxPool1d(3, stride, padding=1), conv(ni, nb_filters, 1))\n",
    "        self.bn_relu = nn.Sequential(nn.BatchNorm1d((len(kss)+1)*nb_filters), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        bottled = self.bottleneck(x)\n",
    "        return self.bn_relu(torch.cat([c(bottled) for c in self.convs]+[self.conv_bottle(x)], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`InceptionModule(64. nb_filters=32)`: will create a 64 channel input module to 3 x 32 channel kernels stacked together with a 32 bottleneck, to form a 128 channel output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionModule(\n",
       "  (bottleneck): Conv1d(8, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "    (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "    (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "  )\n",
       "  (conv_bottle): Sequential(\n",
       "    (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (1): Conv1d(8, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  )\n",
       "  (bn_relu): Sequential(\n",
       "    (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = InceptionModule(8, nb_filters=32)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im(torch.rand(2, 8, 100)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating InceptionTime:\n",
    "- ni: number of input channels\n",
    "- nout: number of outputs, should be equal to the number of classes for classification tasks.\n",
    "- kss: kernel sizes for the inception Block.\n",
    "- bottleneck_size: The number of channels on the convolution bottleneck.\n",
    "- nb_filters: Channels on the convolution of each kernel.\n",
    "- head: True if we want a head attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_inception(ni, nout, kss=[39, 19, 9], depth=6, bottleneck_size=32, nb_filters=32, head=True):\n",
    "    \"Creates an InceptionTime arch from `ni` channels to `nout` outputs.\"\n",
    "    layers = []\n",
    "    n_ks = len(kss) + 1\n",
    "    for d in range(depth):\n",
    "        im = SequentialEx(InceptionModule(ni if d==0 else n_ks*nb_filters, kss=kss, bottleneck_size=bottleneck_size))\n",
    "        if d%3==2: im.append(Shortcut(n_ks*nb_filters, n_ks*nb_filters))      \n",
    "        layers.append(im)\n",
    "    head = [AdaptiveConcatPool1d(), Flatten(), nn.Linear(2*n_ks*nb_filters, nout)] if head else []\n",
    "    return  nn.Sequential(*layers, *head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(1, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(1, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(1, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(1, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (1): Shortcut(\n",
       "        (act_fn): ReLU(inplace=True)\n",
       "        (conv): ConvLayer(\n",
       "          (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (1): Shortcut(\n",
       "        (act_fn): ReLU(inplace=True)\n",
       "        (conv): ConvLayer(\n",
       "          (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): AdaptiveConcatPool1d(\n",
       "    (ap): AdaptiveAvgPool1d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (7): Flatten(full=False)\n",
       "  (8): Linear(in_features=256, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception = create_inception(1, 37)\n",
    "inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception(ts).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_models.ipynb.\n",
      "Converted 03_tabular.ipynb.\n",
      "Converted 04_testing.ipynb.\n",
      "Converted 99_index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
