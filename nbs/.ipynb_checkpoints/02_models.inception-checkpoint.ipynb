{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models.inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from timeseries_fastai.core import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastcore.all import *\n",
    "from fastai2.basics import *\n",
    "from fastai2.torch_core import *\n",
    "from fastai2.layers import *\n",
    "from fastai2.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveConcatPool1d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`.\"\n",
    "    def __init__(self, sz:Optional[int]=None):\n",
    "        \"Output will be 2*sz or 2 if sz is None\"\n",
    "        super().__init__()\n",
    "        self.output_size = sz or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.output_size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.output_size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(64, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Shortcut(Module):\n",
    "    \"Merge a shortcut with the result of the module by adding them. Adds Conv, BN and ReLU\"\n",
    "    def __init__(self, ni, nf, act_fn=act_fn): \n",
    "        self.act_fn=act_fn\n",
    "        self.conv=conv(ni, nf, 1)\n",
    "        self.bn=nn.BatchNorm1d(nf)\n",
    "    def forward(self, x): return act_fn(x + self.bn(self.conv(x.orig)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionModule(Module):\n",
    "    \"An inception module for TimeSeries, based on https://arxiv.org/pdf/1611.06455.pdf\"\n",
    "    def __init__(self, ni, nb_filters=32, kss=[39, 19, 9], bottleneck_size=32, stride=1):\n",
    "        if (bottleneck_size>0 and ni>1): self.bottleneck = conv(ni, bottleneck_size, 1, stride)\n",
    "        else: self.bottleneck = noop\n",
    "        self.convs = nn.ModuleList([conv(bottleneck_size if (bottleneck_size>1 and ni>1) else ni, nb_filters, ks) for ks in kss])\n",
    "        self.conv_bottle = nn.Sequential(nn.MaxPool1d(3, stride, padding=1), conv(ni, nb_filters, 1))\n",
    "        self.bn_relu = nn.Sequential(nn.BatchNorm1d((len(kss)+1)*nb_filters), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        bottled = self.bottleneck(x)\n",
    "        return self.bn_relu(torch.cat([c(bottled) for c in self.convs]+[self.conv_bottle(x)], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionModule(\n",
       "  (bottleneck): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "    (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "    (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "  )\n",
       "  (conv_bottle): Sequential(\n",
       "    (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  )\n",
       "  (bn_relu): Sequential(\n",
       "    (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InceptionModule(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_inception(ni, nout, kss=[39, 19, 9], depth=6, bottleneck_size=32, nb_filters=32, head=True):\n",
    "    \"Inception time architecture\"\n",
    "    layers = []\n",
    "    n_ks = len(kss) + 1\n",
    "    for d in range(depth):\n",
    "        im = SequentialEx(InceptionModule(ni if d==0 else n_ks*nb_filters, kss=kss, bottleneck_size=bottleneck_size))\n",
    "        if d%3==2: im.append(Shortcut(n_ks*nb_filters, n_ks*nb_filters))      \n",
    "        layers.append(im)\n",
    "    head = [AdaptiveConcatPool1d(), Flatten(), nn.Linear(2*n_ks*nb_filters, nout)] if head else []\n",
    "    return  nn.Sequential(*layers, *head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(1, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(1, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(1, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(1, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (1): Shortcut(\n",
       "        (act_fn): ReLU(inplace=True)\n",
       "        (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): SequentialEx(\n",
       "    (layers): ModuleList(\n",
       "      (0): InceptionModule(\n",
       "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
       "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
       "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "        )\n",
       "        (conv_bottle): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (bn_relu): Sequential(\n",
       "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (1): Shortcut(\n",
       "        (act_fn): ReLU(inplace=True)\n",
       "        (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): AdaptiveConcatPool1d(\n",
       "    (ap): AdaptiveAvgPool1d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool1d(output_size=1)\n",
       "  )\n",
       "  (7): Flatten(full=False)\n",
       "  (8): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_inception(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 02_models.inception.ipynb.\n",
      "Converted 99_index.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
